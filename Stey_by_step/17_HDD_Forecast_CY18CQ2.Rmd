---
title: "HDD Forecast CY18CQ2, compared to Lock"
author: "Yao"
date: "August 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(feather)

options(scipen = 999)
library(lubridate)

# Load egboost relevant libraries
library(quantmod); library(TTR); library(xgboost);

# Load timetk package
library(tidyquant)
library(timetk)
library(broom)

# Random Forest
library(party)
library(randomForest)

# Load TBATS package, as well as for ARIMA
library(forecast)
library(forecTheta) # Theta

# Load Facebook Prophet package
library(Rcpp)
library(prophet)

library(scales) # for percent

# make the code parallel using 'parallel' package
library(iterators)
library(parallel)
library(foreach)
library(doParallel)

library(seer)
 
# Calculate the number of cores
no_cores <- detectCores() - 1
registerDoParallel(no_cores)

```

Got Lock Guidance CY18CQ2 Results from Cinkie.

```{r Read the Loack Guidance CY18CQ2 Results}
# Lock Guidance from Cinkie on 8/13/2018
Lock_Guidance_CY18CQ2_Results<- read.csv("~/Yao_Excels/Lock Guidance CY18CQ2 Results(1).csv")
```

9/58 of CFGs' attainment rates were within [0.8,1.2]. In other words, the commodity "Accuracy" is 9/58=15.5%.

```{r Analyze the Accuracy of Lock_Guidance, eval=FALSE, include=FALSE}
Lock_Guidance_CY18CQ2_Results %>% 
  filter(ITM_TYPE=="HDD" & Lock_APE<=5) %>%
  select(Actual,Lock_APE,Interface_w_Speed,Drive_Form_Factor,Drive_Entrypted_Code,HDD_RPM,HDD_Data_Sector_Format,Capacity_Num) %>%
GGally::ggpairs()
```

Low Actual Volume has  some super high APEs. APEs of high volume Actual are relatively lower.

If I want to forecast on Clusters before CFGs, I would use Volume and Interface.

To compare my forecast results to the Lock Guidance, I will forecast for each CFG first.

## HDD Weekly Demand Forecasting for each CFG

```{r load the data}
# # 8/9/2018 New Data from Cinkie
# hdd_qty <- read.csv('~/Yao_Excels/ESG HDD Weekly Volume.csv')
# indx <- c('Fiscal_Wk_End_Date','Fiscal_Wk','Fiscal_Mo','Calendar_Qtr')
# hdd_qty[indx] <- lapply(hdd_qty[indx], function(x) as.character(x))
# 
# # Extracted Needed Weekly data
# commodity.data.selected <- hdd_qty %>% 
#   filter(ESI_Flag=="N"& ITM_TYPE=="HDD") %>% # We don't forecast ESI_Flag=="Y" and I only focus on HDD for now.
#   select(CFG, Calendar_Qtr, Fiscal_Mo, Fiscal_Wk_End_Date,ITM_QTY)

# Use the IRIS data # 08/15/2018 the latest data!
data_part_cfg_wk_in_scope <- read_feather('~/Yao_Rdata/data_part_cfg_wk_in_scope.feather')
commodity.data.selected <- data_part_cfg_wk_in_scope %>%
  filter(ITM_TYPE == 'HDD') %>%
  select(CFG, Calendar_Qtr, Fiscal_Mo, Fiscal_Wk_End_Date,ITM_QTY)

# Extract the Quarter and Month pairs
qtr_mo_tbl <- commodity.data.selected %>% 
  distinct(Calendar_Qtr, Fiscal_Mo) %>% 
  arrange(Fiscal_Mo)
  
```


```{r Forecast Function}
oneCFG_forecast <- function(oneCFG.data,Fcast_Qtrs,MonthAhead = 3, Fcast_Wk_num = 13, forecastPeriodLen = 26){

  MultiQtr_Fcast <- list()
  
  for (Fcast_Qtr in Fcast_Qtrs){
    
    # Split into training and test sets
    train_end_mo = qtr_mo_tbl$Fiscal_Mo[min(which(qtr_mo_tbl$Calendar_Qtr == Fcast_Qtr)) - MonthAhead]
    
    train <- oneCFG.data %>% 
      filter(Fiscal_Mo < train_end_mo) %>%
      mutate(ds = ymd(Fiscal_Wk_End_Date), y = ITM_QTY) %>%
      select(ds,y)

    test <- oneCFG.data %>% 
      filter(Calendar_Qtr == Fcast_Qtr) %>%
      mutate(ds = ymd(Fiscal_Wk_End_Date), y = ITM_QTY) %>%
      select(ds,y)
    
    ## forecast the CFGs that exists in botn train and test set
    ## filter CFGs that have less than 3 historical data points
    
    if (nrow(test) < 1 | nrow(train)<=3){ 
      OneQtrFcast <- c(CFGgroups[i_CFG],Fcast_Qtr,rep(NA,Fcast_Wk_num+1))
      MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
      next
    }
    
    Truth <- c(CFGgroups[i_CFG],Fcast_Qtr,'Actual',test$y)
    
    # Naive Model uses the latest data for forecast.
    Naive <- c(CFGgroups[i_CFG],Fcast_Qtr,'naive',tail(train$y,Fcast_Wk_num))
    
    # Add time series signature # For the last date of train region
    train_augmented <- train[nrow(train),] %>%
      tk_augment_timeseries_signature()
    
    HDD.ts <- ts(train$y,frequency=365.25/7, end = train_augmented$year+train_augmented$yday/365.25)
    
    # TBATS
    fit <- tbats(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    TBATS_fcast_tmp <- tail(fcast$mean,Fcast_Wk_num)
    TBATS_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'TBATS',TBATS_fcast_tmp)
    
    # ARIMA
    fit <- auto.arima(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ARIMA_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'ARIMA',tail(fcast$mean,Fcast_Wk_num))
    
    # Prophet
    m <- prophet(train)
    future <- make_future_dataframe(m, periods = forecastPeriodLen, freq = 'week',include_history = FALSE)
    fcast <- predict(m, future)
    Prophet_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'Prophet',tail(fcast$yhat,Fcast_Wk_num))
    
    # lm
    # Add time series signature
    train_augmented <- train %>%
      tk_augment_timeseries_signature()
    # maunually drop unvalid variables
    temp <-
      train_augmented %>% select(-wday.lbl,-month.lbl,-diff)
    # Remove the column with only one value
    if (nrow(temp)>1) {
      temp <- Filter(function(x)(length(unique(x))>1), temp)
    }
    
    # Model using the augmented features
    fit <- lm(y ~ ., data = na.omit(temp)) # Will need to try using less number of features! Avoid overfitting.
    # We need to again augment the time series signature to the test set.
    test_augmented <- test %>%
      tk_augment_timeseries_signature()
    yhat_test <- predict(fit, newdata = test_augmented)
    lm_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'lm',tail(yhat_test,Fcast_Wk_num))
    
    # Random Forest
    # Model using the augmented features
    fit <- randomForest(y ~ ., data = na.omit(temp))
    yhat_test <- predict(fit, newdata = test_augmented)
    RF_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'RF',tail(yhat_test,Fcast_Wk_num))
    
    # Xgboost
    # Train the xgboost Model using the "xgboost" function
    dtrain = xgb.DMatrix(data = as.matrix(select(temp,-y,-ds)), label = temp$y)
    xgModel = xgboost(data = dtrain, nrounds = 20, verbose = 0) # sqrt(nrow(dtrain)) for classification, ceiling(nrow(dtrain)/3) for regression
    # Make the predictions on the test data
    temp_test <- test_augmented[,colnames(temp)]
    preds = predict(xgModel, as.matrix(select(temp_test,-y,-ds)))
    Xgboost_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'Xgboost',tail(preds,Fcast_Wk_num))
    
    # ets
    fit <- ets(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ets_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'ets',tail(fcast$mean,Fcast_Wk_num))
   
    # rw: random walk
    rw_fit <- rwf(HDD.ts,drift=FALSE, h=forecastPeriodLen)
    forecastRW <- forecast(rw_fit)$mean
    rw_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'rw',tail(forecastRW,Fcast_Wk_num))
    
    # rwd: random walk with drift
    rw_fit <- rwf(HDD.ts,drift=TRUE, h=forecastPeriodLen)
    forecastRWD <- forecast(rw_fit)$mean
    rwd_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'rwd',tail(forecastRWD,Fcast_Wk_num))
    
    # wn: white noise # Calculate accuracy measure based on white noise process
    fit_WN <- auto.arima(HDD.ts, d=0, D=0, max.p=0, max.q = 0,
                     max.Q=0, max.P = 0)
    forecastWN <- forecast(fit_WN,h=forecastPeriodLen)$mean
    wn_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'wn',tail(forecastWN,Fcast_Wk_num))
    
    # theta: standard theta method
    # if (i_Qtr>2) {
    #   forecastTheta <- stheta(HDD.ts,h=forecastPeriodLen, s='additive')$mean
    #   theta_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'theta',tail(forecastTheta,Fcast_Wk_num))
    # }
    # Error in decompose(y, type = s_type) : 
    #   time series has no or less than 2 periods

    # stlar
    forecastSTLAR <- stlar(HDD.ts,h=forecastPeriodLen)$mean
    stlar_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'stlar',tail(forecastSTLAR,Fcast_Wk_num))
    
    # nn
    fit_nnetar <- nnetar(HDD.ts)
    forecastnnetar <- forecast(fit_nnetar, h=forecastPeriodLen)$mean
    nn_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'nn',tail(forecastnnetar,Fcast_Wk_num))

    # snaive
    # forecastSNAIVE <- snaive(HDD.ts, h=forecastPeriodLen)$mean
    
    # mstlarima # This does not work.
    # fit_stlf <- stlf(HDD.ts, method=mtd)
    # forecastMSTL <- forecast(fit_stlf, h=forecastPeriodLen)$mean
    
    # mstlets
    
    OneQtrFcast <- rbind(Truth,Naive,TBATS_fcast,ARIMA_fcast,Prophet_fcast,lm_fcast,RF_fcast,Xgboost_fcast,ets_fcast,rw_fcast,rwd_fcast,wn_fcast,stlar_fcast,nn_fcast)
    
    MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
  }
  return(MultiQtr_Fcast)
}
```


```{r Run the fuctions parallelly}

CFGgroups <- levels(as.factor(commodity.data.selected$CFG))
len <- length(CFGgroups)

Fcast_Qtrs <- c("CY17CQ3","CY17CQ4","CY18CQ1","CY18CQ2") # The quarters that you want to forecast.

Comb_fcast <- foreach(i_CFG=1:len, .combine=rbind,  .packages=c('tidyverse','forecast','forecTheta','seer','Rcpp','ggthemes','lubridate','tidyquant','timetk','prophet','randomForest','party','quantmod','TTR','xgboost')) %dopar% {
  # Forecast for each CFG
  oneCFG.data <- commodity.data.selected %>% filter(CFG==CFGgroups[i_CFG])
  oneCFG_fcast <- oneCFG_forecast(oneCFG.data,Fcast_Qtrs)
  oneCFG_fcast
}

# Assign Colume names
weekNames <- c(sprintf("W0%d", 1:9),sprintf("W%d", 10:13))
dimnames(Comb_fcast)[[2]] <- c('CFG','Quarter','Model',as.character(weekNames))
# HDD quntatity should be numeric.
Comb_fcast[,4:ncol(Comb_fcast)] <- lapply(Comb_fcast[,4:ncol(Comb_fcast)], function(x) as.numeric(as.character(x)))
Comb_fcast[,1:3] <- lapply(Comb_fcast[,1:3], function(x) as.character(x))

# Transformation
Comb_fcast %>% data.frame() %>% gather(key='Fiscal_Wk',value='HDD_QTY',W01:W13) -> All_fcast
All_fcast[,1:4] <- lapply(All_fcast[,1:4], function(x) as.character(x))
All_fcast$HDD_QTY <- as.numeric(as.character(All_fcast$HDD_QTY))
All_fcast$HDD_QTY[All_fcast$HDD_QTY<0] <- 0

# For plot, do not delete the NA for now.
# saveRDS(All_fcast, 'All_fcast_CV.rds') # my_data <- readRDS(file)
write_feather(All_fcast,"~/Yao_Rdata/All_fcast_CV_Cal_Qtr_0815.feather")
write_excel_csv(All_fcast,"~/Yao_Excels/All_fcast_CV_Cal_Qtr_0815.csv")
```

```{r error calculate functions}
calculate_attain = function(y, yhat){
  
  ## attain = actual / forecast
  if (yhat != 0){
    attain = y/yhat
  }else if (y == 0 & yhat == 0){
    attain = 1
  }else if ( y != 0 & yhat == 0){
    attain = 0
  }
  
  return(attain)
}

calculate_ape = function(y, yhat){
  
  ape = ifelse(y == 0 & yhat == 0, 0, abs(y-yhat)/y)
  
  return(ifelse(is.infinite(ape), NA, ape))
}
```


```{r Error Calculation}
yao_hdd = read_feather("~/Yao_Rdata/All_fcast_CV_Cal_Qtr_0815.feather")

result_4q_yao = yao_hdd %>% 
  filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
  group_by(CFG, Model,Quarter) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  ungroup() %>%
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
      group_by(CFG, Model,Quarter) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual')) %>% 
  mutate(APE = calculate_ape(HDD_Actual,Yao_QTY), #abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = calculate_attain(HDD_Actual,Yao_QTY))#HDD_Actual/Yao_QTY) 

result_4q_yao  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) -> result_4q

# Commodity "Accuracy"
# num_CFG <- length(unique(result_4q_yao$CFG))
# result_4q_yao %>%
#   select(CFG,Model,Quarter,AttainmentRate) %>%
#   filter(AttainmentRate>=0.8 & AttainmentRate<=1.2) %>%
#   spread(Model,AttainmentRate) %>%
#   ungroup() %>%
#   select(-CFG,-Quarter) %>%
#   sapply(function(x) length(which(!is.na(x)))/length(x)) %>%
#   data.frame() -> CommodityAccuracy

# Compare to Lock Guidance
yao_hdd %>% 
  filter(Quarter == 'CY18CQ2') %>% 
  group_by(CFG, Model) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter == 'CY18CQ2') %>% 
      group_by(CFG, Model) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual'), by = 'CFG') %>%
  spread(Model,Yao_QTY) -> Yao_HDD_QTY

Lock_Guidance_CY18CQ2_Results %>% 
  filter(ITM_TYPE == 'HDD') %>%
  select(CFG,Lock_Forecast) %>%
  left_join(Yao_HDD_QTY, by = 'CFG') %>%
  gather(Model,HDD_QTY,-CFG,-HDD_Actual) %>%
  filter(!is.na(HDD_Actual)) %>% # only keep the common CFGs
  mutate(APE = calculate_ape(HDD_Actual,HDD_QTY), #abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = calculate_attain(HDD_Actual,HDD_QTY)) -> result_q2

result_q2  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) -> MAPE.results

# Commodity "Accuracy"
result_q2 %>%
  select(CFG,Model,AttainmentRate,HDD_Actual) %>%
  mutate(Attain_Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8~ 'Blue',
    AttainmentRate > 1.2~ 'Red')) %>%
  group_by(Model) %>%
  summarise(Accuracy=length(which(Attain_Flag=="Green"))/length(Attain_Flag),weighted.Accuracy=sum(HDD_Actual[which(Attain_Flag=="Green")])/sum(HDD_Actual)) %>%
  left_join(MAPE.results) -> EvalResults
  
write_excel_csv(EvalResults,"~/Yao_Excels/EvalResults_0815.csv")

# Analyze the errors
# result_q2 %>% 
#   left_join(select(data.joined,CFG,CFG_GROUP,Interface,Interface_w_Speed,Capacity,Drive_Form_Factor,Drive_Entrypted_Code,HDD_RPM,HDD_Data_Sector_Format,Capacity_Num,Avg_Qtr_HDD_QTY,HDD_Volume_Group) %>% unique() %>% arrange(CFG), by = 'CFG') -> Error.w.Attributes

#write_excel_csv(Error.w.Attributes,"~/Yao_Excels/Error_W_Attributes_v2.csv")
```


```{r close the clusters}
# for Parallel package
stopImplicitCluster()
```

```{r Viz the errors}
## compare Models result with LOCK in terms of attainment 
result_q2 %>%
  select(CFG,Model,AttainmentRate,HDD_Actual) %>%
  mutate(Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8~ 'Blue',
    AttainmentRate > 1.2~ 'Red')) %>%
  group_by(Model, Flag) %>% 
  summarise(n_cfg = n_distinct(CFG)) %>% 
  mutate(n_cfg_ptg = n_cfg/sum(n_cfg)) %>% 
  ungroup() %>% 
  mutate(Flag = coalesce(Flag, "NA")) %>% 
  mutate(Flag = ordered(Flag, levels = c("Red", "Green", "Blue", "NA"))) %>% 
  mutate(Model = ordered(Model, levels = c("ARIMA", "Prophet", "stlar", "ets", "naive", "XGBoost", "Lock_Forecast"))) %>%
  filter(!is.na(Model)) %>%
  ggplot(aes(x = Model, y = n_cfg_ptg, fill = Flag)) +
  geom_bar(stat = 'identity') +
  geom_text(aes(label = paste0(round(100*n_cfg_ptg), '%')), colour = 'white', fontface = 'bold',
            size = 4, position = position_stack(), vjust = 1) +
  scale_fill_manual(values = c("#E15759", "#59A14F","#4E79A7", "grey")) +
  scale_y_continuous(labels = scales::percent) +
  theme_hc() +
  theme(axis.text.x = element_text(size = 8)) +
  labs(x = '', y = '% CFG Quantity', title = "Model Attainment and 'Accuracy'", subtitle = "CY18CQ2; Forecast 1 quarter ahead")

```

