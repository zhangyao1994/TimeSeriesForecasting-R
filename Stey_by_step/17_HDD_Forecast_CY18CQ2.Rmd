---
title: "HDD Forecast Cross-Validation and CY18CQ2 compared to Lock"
author: "Yao"
date: "August 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(feather)

options(scipen = 999)
library(lubridate)

# Load egboost relevant libraries
library(quantmod); library(TTR); library(xgboost);

# Load timetk package
library(tidyquant)
library(timetk)
library(broom)

# Random Forest
library(party)
library(randomForest)

# Load TBATS package, as well as for ARIMA
library(forecast)
library(forecTheta) # Theta

# Load Facebook Prophet package
library(Rcpp)
library(prophet)

library(scales) # for percent

# make the code parallel using 'parallel' package
library(iterators)
library(parallel)
library(foreach)
library(doParallel)

library(seer)
 
# Calculate the number of cores
no_cores <- detectCores() - 1
registerDoParallel(no_cores)

```

Got Lock Guidance CY18CQ2 Results from Cinkie.

```{r Read the Loack Guidance CY18CQ2 Results}
# Lock Guidance from Cinkie on 8/13/2018 but only trust the Lock_FOrecast please.
Lock_Guidance_CY18CQ2_Results<- read.csv("~/Yao_Excels/Lock Guidance CY18CQ2 Results(1).csv")
```

```{r Analyze the Accuracy of Lock_Guidance, eval=FALSE, include=FALSE}
# The Actual in this csv file is not reliable.
Lock_Guidance_CY18CQ2_Results %>% 
  filter(ITM_TYPE=="HDD" & Lock_APE<=5) %>%
  select(Actual,Lock_APE,Interface_w_Speed,Drive_Form_Factor,Drive_Entrypted_Code,HDD_RPM,HDD_Data_Sector_Format,Capacity_Num) %>%
GGally::ggpairs()
```

Low Actual Volume has  some super high APEs. APEs of high volume Actual are relatively lower.

If I want to forecast on Clusters before CFGs, I would use Volume and Interface.

To compare my forecast results to the Lock Guidance, I will forecast for each CFG first.

## HDD Weekly Demand Forecasting for each CFG

```{r load the data}
# Use the IRIS data # 08/15/2018 the latest data!
data_part_cfg_wk_in_scope <- read_feather('~/Yao_Rdata/data_part_cfg_wk_in_scope.feather')
commodity.data.selected <- data_part_cfg_wk_in_scope %>%
  filter(ITM_TYPE == 'HDD') %>%
  select(CFG, Calendar_Qtr, Fiscal_Mo, Fiscal_Wk_End_Date,ITM_QTY)

# Extract the Quarter and Month pairs
qtr_mo_tbl <- commodity.data.selected %>% 
  distinct(Calendar_Qtr, Fiscal_Mo) %>% 
  arrange(Fiscal_Mo)
  
```


```{r Forecast Function}
oneCFG_forecast <- function(oneCFG.data,Fcast_Qtrs,MonthAhead = 3, Fcast_Wk_num = 13, forecastPeriodLen = 26){

  MultiQtr_Fcast <- list()
  
  for (Fcast_Qtr in Fcast_Qtrs){
    
    # ----------------------Split into training and test sets---------------------- ######
    train_end_mo = qtr_mo_tbl$Fiscal_Mo[min(which(qtr_mo_tbl$Calendar_Qtr == Fcast_Qtr)) - MonthAhead]
    
    train <- oneCFG.data %>% 
      filter(Fiscal_Mo < train_end_mo) %>%
      mutate(ds = ymd(Fiscal_Wk_End_Date), y = ITM_QTY) %>%
      select(ds,y)

    test <- oneCFG.data %>% 
      filter(Calendar_Qtr == Fcast_Qtr) %>%
      mutate(ds = ymd(Fiscal_Wk_End_Date), y = ITM_QTY) %>%
      select(ds,y)
    
    ## forecast the CFGs that exists in botn train and test set
    ## filter CFGs that have less than 3 historical data points
    if (nrow(test) < 1 | nrow(train)<=3){ 
      OneQtrFcast <- c(CFGgroups[i_CFG],Fcast_Qtr,rep(NA,Fcast_Wk_num+1))
      MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
      next
    }
    
    Truth <- c(CFGgroups[i_CFG],Fcast_Qtr,'Actual',test$y)
    
    # ----------------------Naive Model uses the latest data for forecast---------------------- ######
    Naive <- c(CFGgroups[i_CFG],Fcast_Qtr,'naive',tail(train$y,Fcast_Wk_num))
    
    # Add time series signature # For the last date of train region
    train_augmented <- train[nrow(train),] %>%
      tk_augment_timeseries_signature()
    
    HDD.ts <- ts(train$y,frequency=365.25/7, end = train_augmented$year+train_augmented$yday/365.25)
    
    # ----------------------TBATS---------------------- ######
    fit <- tbats(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    TBATS_fcast_tmp <- tail(fcast$mean,Fcast_Wk_num)
    TBATS_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'TBATS',TBATS_fcast_tmp)
    
    # ----------------------ARIMA---------------------- ######
    fit <- auto.arima(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ARIMA_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'ARIMA',tail(fcast$mean,Fcast_Wk_num))
    
    # ----------------------Prophet---------------------- ######
    m <- prophet(train)
    future <- make_future_dataframe(m, periods = forecastPeriodLen, freq = 'week',include_history = FALSE)
    fcast <- predict(m, future)
    Prophet_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'Prophet',tail(fcast$yhat,Fcast_Wk_num))
    
    # ----------------------lm---------------------- ######
    # Add time series signature
    train_augmented <- train %>%
      tk_augment_timeseries_signature()
    # maunually drop unvalid variables
    temp <-
      train_augmented %>% select(-wday.lbl,-month.lbl,-diff)
    # Remove the column with only one value
    if (nrow(temp)>1) {
      temp <- Filter(function(x)(length(unique(x))>1), temp)
    }
    
    # Model using the augmented features
    fit <- lm(y ~ ., data = na.omit(temp)) # Will need to try using less number of features! Avoid overfitting.
    # We need to again augment the time series signature to the test set.
    test_augmented <- test %>%
      tk_augment_timeseries_signature()
    yhat_test <- predict(fit, newdata = test_augmented)
    lm_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'lm',tail(yhat_test,Fcast_Wk_num))
    
    # ----------------------Random Forest---------------------- ######
    # Model using the augmented features
    fit <- randomForest(y ~ ., data = na.omit(temp))
    yhat_test <- predict(fit, newdata = test_augmented)
    RF_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'RF',tail(yhat_test,Fcast_Wk_num))
    
    # ----------------------Xgboost----------------------######
    # Train the xgboost Model using the "xgboost" function
    dtrain = xgb.DMatrix(data = as.matrix(select(temp,-y,-ds)), label = temp$y)
    xgModel = xgboost(data = dtrain, nrounds = 20, verbose = 0) # sqrt(nrow(dtrain)) for classification, ceiling(nrow(dtrain)/3) for regression
    # Make the predictions on the test data
    temp_test <- test_augmented[,colnames(temp)]
    preds = predict(xgModel, as.matrix(select(temp_test,-y,-ds)))
    Xgboost_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'Xgboost',tail(preds,Fcast_Wk_num))
    
    # ----------------------ets---------------------- ######
    fit <- ets(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ets_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'ets',tail(fcast$mean,Fcast_Wk_num))
   
    # ----------------------rw: random walk---------------------- ######
    rw_fit <- rwf(HDD.ts,drift=FALSE, h=forecastPeriodLen)
    forecastRW <- forecast(rw_fit)$mean
    rw_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'rw',tail(forecastRW,Fcast_Wk_num))
    
    # ----------------------rwd: random walk with drift---------------------- ######
    rw_fit <- rwf(HDD.ts,drift=TRUE, h=forecastPeriodLen)
    forecastRWD <- forecast(rw_fit)$mean
    rwd_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'rwd',tail(forecastRWD,Fcast_Wk_num))
    
    # ----------------------wn: white noise process---------------------- ######
    # Calculate accuracy measure based on white noise
    fit_WN <- auto.arima(HDD.ts, d=0, D=0, max.p=0, max.q = 0,
                     max.Q=0, max.P = 0)
    forecastWN <- forecast(fit_WN,h=forecastPeriodLen)$mean
    wn_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'wn',tail(forecastWN,Fcast_Wk_num))
    
    # theta: standard theta method
    # if (i_Qtr>2) {
    #   forecastTheta <- stheta(HDD.ts,h=forecastPeriodLen, s='additive')$mean
    #   theta_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'theta',tail(forecastTheta,Fcast_Wk_num))
    # }
    # Error in decompose(y, type = s_type) : 
    #   time series has no or less than 2 periods

    # ----------------------stlar---------------------- ######
    forecastSTLAR <- stlar(HDD.ts,h=forecastPeriodLen)$mean
    stlar_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'stlar',tail(forecastSTLAR,Fcast_Wk_num))
    
    # ----------------------nn---------------------- ######
    fit_nnetar <- nnetar(HDD.ts)
    forecastnnetar <- forecast(fit_nnetar, h=forecastPeriodLen)$mean
    nn_fcast <- c(CFGgroups[i_CFG],Fcast_Qtr,'nn',tail(forecastnnetar,Fcast_Wk_num))

    # snaive
    # forecastSNAIVE <- snaive(HDD.ts, h=forecastPeriodLen)$mean
    
    # mstlarima # This does not work.
    # fit_stlf <- stlf(HDD.ts, method=mtd)
    # forecastMSTL <- forecast(fit_stlf, h=forecastPeriodLen)$mean
    
    # mstlets
    
    OneQtrFcast <- rbind(Truth,Naive,TBATS_fcast,ARIMA_fcast,Prophet_fcast,lm_fcast,RF_fcast,Xgboost_fcast,ets_fcast,rw_fcast,rwd_fcast,wn_fcast,stlar_fcast,nn_fcast)
    
    MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
  }
  return(MultiQtr_Fcast)
}
```


```{r Run the fuctions parallelly}

CFGgroups <- levels(as.factor(commodity.data.selected$CFG))
len <- length(CFGgroups)

Fcast_Qtrs <- c("CY17CQ1","CY17CQ2","CY17CQ3","CY17CQ4","CY18CQ1","CY18CQ2","CY18CQ3","CY18CQ4") # The quarters that you want to forecast.

Comb_fcast <- foreach(i_CFG=1:len, .combine=rbind,  .packages=c('tidyverse','forecast','forecTheta','seer','Rcpp','ggthemes','lubridate','tidyquant','timetk','prophet','randomForest','party','quantmod','TTR','xgboost')) %dopar% {
  # Forecast for each CFG
  oneCFG.data <- commodity.data.selected %>% filter(CFG==CFGgroups[i_CFG])
  oneCFG_fcast <- oneCFG_forecast(oneCFG.data,Fcast_Qtrs)
  oneCFG_fcast
}

# Assign Colume names
weekNames <- c(sprintf("W0%d", 1:9),sprintf("W%d", 10:13))
Comb_fcast <- Comb_fcast %>% data.frame()
dimnames(Comb_fcast)[[2]] <- c('CFG','Quarter','Model',as.character(weekNames))
# HDD quntatity should be numeric.
Comb_fcast[,4:ncol(Comb_fcast)] <- lapply(Comb_fcast[,4:ncol(Comb_fcast)], function(x) as.numeric(as.character(x)))
Comb_fcast[,1:3] <- lapply(Comb_fcast[,1:3], function(x) as.character(x))

# Transformation
Comb_fcast %>% gather(key='Fiscal_Wk',value='HDD_QTY',W01:W13) -> All_fcast
All_fcast$HDD_QTY[All_fcast$HDD_QTY<0] <- 0

# For plot, do not delete the NA for now.
# saveRDS(All_fcast, 'All_fcast_CV.rds') # my_data <- readRDS(file)
write_feather(All_fcast,"~/Yao_Rdata/All_fcast_CV_Cal_Qtr_0816.feather")
write_excel_csv(All_fcast,"~/Yao_Excels/All_fcast_CV_Cal_Qtr_0816.csv")
```

```{r error calculate functions}
calculate_attain = function(y, yhat){
  
  ## attain = actual / forecast
  if (yhat != 0){
    attain = y/yhat
  }else if (y == 0 & yhat == 0){
    attain = 1
  }else if ( y != 0 & yhat == 0){
    attain = 0
  }
  
  return(ifelse(is.infinite(attain), NA, attain))
}

calculate_ape = function(y, yhat){
  
  ape = ifelse(y == 0 & yhat == 0, 0, abs(y-yhat)/y)
  
  return(ifelse(is.infinite(ape), NA, ape))
}
```


```{r Error Calculation}
yao_hdd = read_feather("~/Yao_Rdata/All_fcast_CV_Cal_Qtr_0816.feather")

# All quarters from CY17CQ1-CY18CQ2 ####
result_yao = yao_hdd %>% 
  # filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
  group_by(CFG, Model,Quarter) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  ungroup() %>%
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      # filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
      group_by(CFG, Model,Quarter) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual')) %>% 
  mutate(APE = calculate_ape(HDD_Actual,Yao_QTY), #abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = calculate_attain(HDD_Actual,Yao_QTY))#HDD_Actual/Yao_QTY) 

result_yao  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) 

# MAPE and weighted.MAPE #####
result_yao  %>%
  group_by(Model,Quarter) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) %>%
  gather(Metrics,values,MAPE,weighted.MAPE) %>%
  ungroup() %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive')) %>%
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive"))) %>%
  ggplot(aes(x = Model , y = values, fill = Metrics)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  geom_text(aes(label = paste0(round(100*values), '%')), colour = 'grey', fontface = 'bold',
            size = 3.5, position = position_dodge(width = 0.8), vjust = 1) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_economist() +
  theme_bw(base_size = 14) +
  labs(x = '', y = "Errors",
       title = paste("HDD Quarterly CFG MAPE (",length(unique(result_4q_yao$CFG)),'CFGs included )'),fill = "Metrics") +
  facet_wrap(~Quarter)

# 4 quarters from CY17CQ3-CY18CQ2 ####
result_4q_yao = yao_hdd %>% 
  filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
  group_by(CFG, Model,Quarter) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  ungroup() %>%
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
      group_by(CFG, Model,Quarter) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual')) %>% 
  mutate(APE = calculate_ape(HDD_Actual,Yao_QTY), #abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = calculate_attain(HDD_Actual,Yao_QTY))#HDD_Actual/Yao_QTY) 

# MAPE and weighted.MAPE #####

result_4q_yao  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) %>%
  gather(Metrics,values,MAPE,weighted.MAPE) %>%
  ungroup() %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive')) %>%
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive"))) %>%
  ggplot(aes(x = Model , y = values, fill = Metrics)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  geom_text(aes(label = paste0(round(100*values), '%')), colour = 'grey', fontface = 'bold',
            size = 3.5, position = position_dodge(width = 0.8), vjust = 1) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_economist() +
  theme_bw(base_size = 14) +
  labs(x = '', y = "Errors",
       title = paste("HDD Quarterly CFG MAPE"),subtitle = paste("CY17CQ3-CY18CQ2; Forecast 1 quarter ahead (",length(unique(result_4q_yao$CFG)),'CFGs included )'),fill = "Metrics")

# By quarter
result_4q_yao  %>%
  group_by(Model,Quarter) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) %>%
  gather(Metrics,values,MAPE,weighted.MAPE) %>%
  ungroup() %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive')) %>%
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive"))) %>%
  ggplot(aes(x = Model , y = values, fill = Metrics)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  geom_text(aes(label = paste0(round(100*values), '%')), colour = 'grey', fontface = 'bold',
            size = 3.5, position = position_dodge(width = 0.8), vjust = 1) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_economist() +
  theme_bw(base_size = 14) +
  labs(x = '', y = "Errors",
       title = paste("HDD Quarterly CFG MAPE"),subtitle = paste("Forecast 1 quarter ahead (",length(unique(result_4q_yao$CFG)),'CFGs included )'),fill = "Metrics") +
  facet_wrap(~Quarter)

# Commodity "Accuracy" #####
result_4q_yao  %>%
 select(CFG,Model ,AttainmentRate,HDD_Actual,Quarter) %>%
  group_by(Model) %>%
  mutate(Attain_Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8~ 'Blue',
    AttainmentRate > 1.2~ 'Red')) %>%
  summarise(Accuracy=length(which(Attain_Flag=="Green"))/length(Attain_Flag),weighted.Accuracy=sum(HDD_Actual[which(Attain_Flag=="Green")])/sum(HDD_Actual)) -> Accuracy.4q

result_4q_yao  %>%
  select(CFG,Model,AttainmentRate,HDD_Actual) %>%
  mutate(Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8 ~ 'Blue',
    AttainmentRate > 1.2 ~ 'Red')) %>%
  group_by(Model, Flag) %>% 
  summarise(n_cfg = n_distinct(CFG)) %>% 
  mutate(n_cfg_ptg = n_cfg/sum(n_cfg)) %>% 
  ungroup() %>% 
  mutate(Flag = coalesce(Flag, "NA")) %>% 
  mutate(Flag = ordered(Flag, levels = c( "NA","Red", "Green", "Blue"))) %>% 
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive"))) %>%
  filter(!is.na(Model)) %>%
  ggplot(aes(x = Model, y = n_cfg_ptg, fill = Flag)) +
  geom_bar(stat = 'identity') +
  geom_text(aes(label = paste0(round(100*n_cfg_ptg), '%')), colour = 'white', fontface = 'bold',
            size = 4, position = position_stack(), vjust = 1) +
  scale_fill_manual(values = c( "grey","#E15759", "#59A14F","#4E79A7")) +
  scale_y_continuous(labels = scales::percent) +
  theme_hc() +
  theme(axis.text.x = element_text(size = 14)) +
  labs(x = '', y = '% CFG Quantity', title = "Model Attainment and 'Accuracy'", subtitle = paste("CY17CQ3-CY18CQ2; Forecast 1 quarter ahead (",length(unique(result_4q_yao$CFG)),'CFGs included )'))

# By quarter
result_4q_yao  %>%
  select(CFG,Model,AttainmentRate,HDD_Actual,Quarter) %>%
  mutate(Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8 ~ 'Blue',
    AttainmentRate > 1.2 ~ 'Red')) %>%
  group_by(Model, Quarter, Flag) %>% 
  summarise(n_cfg = n_distinct(CFG)) %>% 
  mutate(n_cfg_ptg = n_cfg/sum(n_cfg)) %>% 
  ungroup() %>% 
  mutate(Flag = coalesce(Flag, "NA")) %>% 
  mutate(Flag = ordered(Flag, levels = c( "NA","Red", "Green", "Blue"))) %>% 
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive"))) %>%
  filter(!is.na(Model)) %>%
  ggplot(aes(x = Model, y = n_cfg_ptg, fill = Flag)) +
  geom_bar(stat = 'identity') +
  geom_text(aes(label = paste0(round(100*n_cfg_ptg), '%')), colour = 'white', fontface = 'bold',
            size = 4, position = position_stack(), vjust = 1) +
  scale_fill_manual(values = c( "grey","#E15759", "#59A14F","#4E79A7")) +
  scale_y_continuous(labels = scales::percent) +
  theme_hc() +
  theme(axis.text.x = element_text(size = 14)) +
  labs(x = '', y = '% CFG Quantity', title = "Model Attainment and 'Accuracy'", subtitle = paste("Forecast 1 quarter ahead (",length(unique(result_4q_yao$CFG)),'CFGs included )')) +
  facet_wrap(~Quarter)

# Compare to Lock Guidance in CY18CQ2 ####
yao_hdd %>% 
  filter(Quarter == 'CY18CQ2') %>% 
  group_by(CFG, Model) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter == 'CY18CQ2') %>% 
      group_by(CFG, Model) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual'), by = 'CFG') %>%
  spread(Model,Yao_QTY) -> Yao_HDD_QTY

Lock_Guidance_CY18CQ2_Results %>% 
  filter(ITM_TYPE == 'HDD') %>%
  select(CFG,Lock_Forecast) %>%
  full_join(Yao_HDD_QTY, by = 'CFG') %>%
  filter(!is.na(HDD_Actual)) %>%
  gather(Model,HDD_QTY,-CFG,-HDD_Actual) %>%
  mutate(APE = calculate_ape(HDD_Actual,HDD_QTY), #abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = calculate_attain(HDD_Actual,HDD_QTY)) -> result_q2

Lock_Guidance_CY18CQ2_Results %>% 
  filter(ITM_TYPE == 'HDD') %>%
  select(CFG,Lock_Forecast) %>%
  full_join(Yao_HDD_QTY, by = 'CFG') %>%
  filter(!is.na(HDD_Actual) & !is.na(Lock_Forecast)) %>%
  gather(Model,HDD_QTY,-CFG,-HDD_Actual) %>%
  mutate(APE = calculate_ape(HDD_Actual,HDD_QTY), #abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = calculate_attain(HDD_Actual,HDD_QTY))  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) -> MAPE.results

# MAPE and WMAPE
MAPE.results %>%
  gather(Metrics,values,MAPE,weighted.MAPE) %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive', "Lock_Forecast")) %>%
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive", "Lock_Forecast"))) %>%
  ggplot(aes(x = Model , y = values, fill = Metrics)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  geom_text(aes(label = paste0(round(100*values), '%')), colour = 'grey', fontface = 'bold',
            size = 3.5, position = position_dodge(width = 0.8), vjust = 1) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_economist() +
  theme_bw(base_size = 14) +
  labs(x = '', y = "Errors",
       title = paste("HDD Quarterly CFG MAPE"), subtitle = paste("CY18CQ2; Forecast 1 quarter ahead"),fill = "Metrics")

# Commodity "Accuracy"
result_q2 %>%
  select(CFG,Model,AttainmentRate,HDD_Actual) %>%
  mutate(Attain_Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8~ 'Blue',
    AttainmentRate > 1.2~ 'Red')) %>%
  group_by(Model) %>%
  summarise(Accuracy=length(which(Attain_Flag=="Green"))/length(Attain_Flag),weighted.Accuracy=sum(HDD_Actual[which(Attain_Flag=="Green")])/sum(HDD_Actual)) %>%
  left_join(MAPE.results) -> EvalResults

## compare Models result with LOCK in terms of attainment 
result_q2 %>%
  select(CFG,Model,AttainmentRate,HDD_Actual) %>%
  mutate(Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8 ~ 'Blue',
    AttainmentRate > 1.2 ~ 'Red')) %>%
  group_by(Model, Flag) %>% 
  summarise(n_cfg = n_distinct(CFG)) %>% 
  mutate(n_cfg_ptg = n_cfg/sum(n_cfg)) %>% 
  ungroup() %>% 
  mutate(Flag = coalesce(Flag, "NA")) %>% 
  mutate(Flag = ordered(Flag, levels = c("NA","Red", "Green", "Blue"))) %>% 
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive", "Lock_Forecast"))) %>%
  filter(!is.na(Model)) %>%
  ggplot(aes(x = Model, y = n_cfg_ptg, fill = Flag)) +
  geom_bar(stat = 'identity') +
  geom_text(aes(label = paste0(round(100*n_cfg_ptg), '%')), colour = 'white', fontface = 'bold',
            size = 4, position = position_stack(), vjust = 1) +
  scale_fill_manual(values = c("grey","#E15759", "#59A14F","#4E79A7")) +
  scale_y_continuous(labels = scales::percent) +
  theme_hc(base_size = 14) +
  theme(axis.text.x = element_text(size = 14)) +
  labs(x = '', y = '% CFG Quantity', title = "Model Attainment and 'Accuracy'", subtitle = paste("CY18CQ2; Forecast 1 quarter ahead (",length(unique(result_q2$CFG)),'CFGs included )'))
  
write_excel_csv(EvalResults,"~/Yao_Excels/EvalResults_0816.csv")
```


```{r close the clusters}
# for Parallel package
stopImplicitCluster()
```

```{r Analyze the errors}
## relationship between volume and ape ####
result_q2 %>%
  ungroup() %>% 
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive", "Lock_Forecast"))) %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive', "Lock_Forecast")) %>%
  ggplot(aes(x = HDD_Actual, y = APE)) +
  facet_wrap(~Model) +
  geom_point() +
  scale_y_continuous(labels = scales::percent, limits = c(0,5)) +
  scale_x_continuous(labels = scales::comma) +
  theme_classic(base_size = 14) +
  labs(x = "CFG Total Volume", y = "Absolute Percentage Error",
       title = "Scatterplot of CFG volume vs error rate", subtitle = "Test quarter CY18CQ2; Forecast 1 quarter ahead",
       caption = "Error rate > 500% excluded from chart")

## relationship between volume and attainment rate # I do not like this plot.
result_q2 %>%
  ungroup() %>% 
  mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive", "Lock_Forecast"))) %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive', "Lock_Forecast")) %>%
  ggplot(aes(x = HDD_Actual, y = AttainmentRate)) +
  facet_wrap(~Model) +
  geom_point() +
  scale_y_continuous(labels = scales::percent, limits = c(0,5)) +
  scale_x_continuous(labels = scales::comma) +
  theme_classic(base_size = 14) +
  labs(x = "CFG Total Volume", y = "Attainment Rate",
       title = "Scatterplot of CFG volume vs error rate", subtitle = "Test quarter CY18CQ2; Forecast 1 quarter ahead",
       caption = "Error rate > 500% excluded from chart")

# results joined with Attributes ####
cfg_attr_group = read.csv('~/Yao_Excels/HDD SSD Memory CFG Attributes.csv')
result_q2 %>% 
  left_join(cfg_attr_group %>% select(CFG, Interface, Interface_w_Speed, Capacity, Drive_Form_Factor, Drive_Entrypted_Code, HDD_RPM, HDD_Data_Sector_Format)) -> result_q2.wAttr

## relationship between Interface and ape ####
result_q2.wAttr %>%
    mutate(Model = ordered(Model, levels = c("ARIMA", "ets","Prophet", "RF", "stlar", "wn", "Xgboost", "naive", "Lock_Forecast"))) %>%
  filter(Model %in% c('ARIMA','ets','Prophet','RF','stlar','wn','Xgboost','naive', "Lock_Forecast")) %>%
  ggplot(aes(x = Interface, y = APE)) +
  facet_wrap(~Model) +
  geom_dotplot(binaxis = "y", stackdir = "center") +
  scale_y_continuous(labels = scales::percent, limits = c(0,5)) +
  theme_classic(base_size = 14) +
  labs(x = "CFG Interface", y = "Absolute Percentage Error",
       title = "Scatterplot of CFG Interface vs error rate", subtitle = "Test quarter CY18CQ2; Forecast 1 quarter ahead",
       caption = "Error rate > 500% excluded from chart")

# Have a glance at all attributes ####
result_q2.wAttr %>%
  filter(Model %in% c('Lock_Forecast')) %>%
  filter(APE<=5) %>%
  mutate(Capacity = as.numeric(Capacity)) %>%
  select(HDD_Actual,APE,AttainmentRate,Interface_w_Speed,Drive_Form_Factor,Drive_Entrypted_Code,HDD_RPM,HDD_Data_Sector_Format,Capacity) %>%
GGally::ggpairs()

# Lock_Forecast shows different performance on different Interface_w_Speed, Data_Entrypted_Code, and Data_Sector_Format, but Xgboost does not.
```

