---
title: "HDD Forecast CY18CQ2"
author: "Yao"
date: "August 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(feather)
```

Got Lock Guidance CY18CQ2 Results from Cinkie.

```{r Read the Loack Guidance CY18CQ2 Results}
library(readr)
Lock_Guidance_CY18CQ2_Results <- read_csv("~/Yao_Excels/Lock Guidance CY18CQ2 Results.csv")
```

```{r Analyze the Accuracy}
Lock_Guidance_CY18CQ2_Results %>% 
  filter(ITM_TYPE=="HDD") %>%
  select(Actual,APE,Interface_w_Speed,Drive_Form_Factor,Drive_Entrypted_Code,HDD_RPM,HDD_Data_Sector_Format,Capacity_Num) %>%
GGally::ggpairs() -> p
```

Low Actual Volume has  some super high APEs. APEs of high volume Actual are relatively lower.

Let me downlaod the data from IRIS and use them to forecast HDD demand for calendar quarter.

## Load Database from SQL Server

```{r ConnectDB}
library(DBI)
db = dbConnect(odbc::odbc(),
               driver = 'SQL Server',
               server = 'IRISAGL01.aus.amer.dell.com',
               user = 'Yao_Z',
               password = 'y67uhgt$Y')
```

Download the latest HDD sales data from SQL server.

The following query was from Cinkie.
```{sql hdd_qty, connection=db, output.var = 'hdd_qty_updated0731'}
SELECT 
D.Fiscal_Yr, D.Calendar_Qtr, D.Fiscal_Mo,D.Fiscal_Wk, D.Fiscal_Wk_End_Date, C.Cfg_Desc AS CFG,
CASE WHEN FMLY_PFOLIO_DESC LIKE '%DSS%' OR FMLY_PFOLIO_DESC LIKE '%DCS%' THEN 'PowerEdge - ESI' ELSE LOB_DESC END AS LOB_DESC, BRAND_CATG_DESC,
SUM(ITM_QTY) AS ITM_QTY
FROM [IRIS].[Base].[ISG_Business_Transformation.isgordersdetails] AS A
LEFT JOIN IRIS_Data_Mart.dbo.SKU_CFG_Bridge AS C
ON A.ITM_NBR = C.sku_num
JOIN IRIS.DIM.date_hyperion_v AS D
ON A.ORD_DT = D.Fiscal_Date
WHERE Commodity_Desc = 'Hard Drive'
AND DELL_EMC_ORDER_FLAG = 'DELL' AND TYPE_DESC = 'Enterprise Solution Group PBU' AND Fiscal_Yr >= 'FY17'
GROUP BY 
D.Fiscal_Yr, D.Calendar_Qtr, D.Fiscal_Mo,D.Fiscal_Wk, D.Fiscal_Wk_End_Date,
C.Cfg_Desc,
CASE WHEN FMLY_PFOLIO_DESC LIKE '%DSS%' OR FMLY_PFOLIO_DESC LIKE '%DCS%' THEN 'PowerEdge - ESI' ELSE LOB_DESC END, BRAND_CATG_DESC
```

```{r Check the CFGs}
unique(hdd_qty_updated0731$CFG) %>% length()
```

I would like to add [AND C.Cfg_Desc LIKE '%HDD%'].

```{sql hdd_qty, connection=db, output.var = 'hdd_qty_updated0731_HDD'}
SELECT 
D.Fiscal_Yr, D.Calendar_Qtr, D.Fiscal_Mo,D.Fiscal_Wk, D.Fiscal_Wk_End_Date, C.Cfg_Desc AS CFG,
CASE WHEN FMLY_PFOLIO_DESC LIKE '%DSS%' OR FMLY_PFOLIO_DESC LIKE '%DCS%' THEN 'PowerEdge - ESI' ELSE LOB_DESC END AS LOB_DESC, BRAND_CATG_DESC,
SUM(ITM_QTY) AS ITM_QTY
FROM [IRIS].[Base].[ISG_Business_Transformation.isgordersdetails] AS A
LEFT JOIN IRIS_Data_Mart.dbo.SKU_CFG_Bridge AS C
ON A.ITM_NBR = C.sku_num
JOIN IRIS.DIM.date_hyperion_v AS D
ON A.ORD_DT = D.Fiscal_Date
WHERE Commodity_Desc = 'Hard Drive' AND C.Cfg_Desc LIKE '%HDD%'
AND DELL_EMC_ORDER_FLAG = 'DELL' AND TYPE_DESC = 'Enterprise Solution Group PBU' AND Fiscal_Yr >= 'FY17'
GROUP BY 
D.Fiscal_Yr, D.Calendar_Qtr, D.Fiscal_Mo,D.Fiscal_Wk, D.Fiscal_Wk_End_Date,
C.Cfg_Desc,
CASE WHEN FMLY_PFOLIO_DESC LIKE '%DSS%' OR FMLY_PFOLIO_DESC LIKE '%DCS%' THEN 'PowerEdge - ESI' ELSE LOB_DESC END, BRAND_CATG_DESC
```

```{r Check the CFGs}
unique(hdd_qty_updated0731_HDD$CFG) %>% length()
```

```{r Compre two versions of hdd_qty using two queries}
hdd_qty <- read_feather('~/Yao_Rdata/HDD_QTY_IRIS.feather') # They query I used till 07/31/2018
week_data <- hdd_qty %>% group_by(Fiscal_Wk) %>%
  summarise(Fiscal_Wk_QTY=sum(PART_QTY))
week_data$version <- 'Yao Current'

week_data.2 <- hdd_qty_updated0731_HDD %>% group_by(Fiscal_Wk) %>%
  summarise(Fiscal_Wk_QTY=sum(ITM_QTY))
week_data.2$version <- 'Cinkie Updated'

week_data %>% full_join(week_data.2) -> week_data.compare

p_week <- week_data.compare %>%
  ggplot(aes(x=Fiscal_Wk,y=Fiscal_Wk_QTY,group=version,color=version)) +
  geom_point(size = 2) +
  geom_line(size = 1.5) +
  labs(title = 'Overall HDD Weekly Sales', x = "Fiscal Week", y = "Part Quantity") + 
  theme_minimal(base_size = 18) + 
  scale_color_tableau('tableau10medium') + 
  scale_x_discrete(breaks = c('FY17W01', 'FY18W01', 'FY19W01','FY20W01')) +
  theme(legend.position = 'bottom',plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(label=comma) + expand_limits(y = 0) #+ coord_cartesian(ylim = c(0,2100000))
p_week

```

They are not that different! Since I need the Calendar_Qtr, use the hdd_qty_updated0731_HDD then.

```{r Deal with missing data}
Weekly_HDD_QTY_combined <- hdd_qty_updated0731_HDD %>% group_by(Fiscal_Wk,Fiscal_Wk) %>% summarise(Fiscal_Wk_QTY=sum(ITM_QTY))
# 7/20/2018 128 weeks

apply(hdd_qty_updated0731_HDD, 2, function(x) any(is.na(x)))

write_feather(hdd_qty_updated0731_HDD,'~/Yao_Rdata/HDD_QTY_IRIS_0731.feather')
```

```{r Load CFG attributes and save the data for Power BI}
library(readr)
HDD_SSD_Memory_CFG_Attributes <- read_csv("~/Yao_Excels/HDD SSD Memory CFG Attributes.csv")
hdd_qty_updated0731_HDD <- read_feather('~/Yao_Rdata/HDD_QTY_IRIS_0731.feather')

# HDD Weekly Sales
week_data <- hdd_qty_updated0731_HDD %>% left_join(HDD_SSD_Memory_CFG_Attributes)

write_excel_csv(week_data,'~/Yao_Excels/HDD_CFG_wk.csv')
```

To create clusters, I will use Region and Volume to classify CFGs. I thought that the "Small and medium" are more stable so that it could be easier to forecast. Actually, "strategic and large" groups have lower errors. This is because that it is easier to forcast large volume compared to low volume.

```{r By Volume}
HDD_data <- read_feather('~/Yao_Rdata/HDD_QTY_IRIS_0731.feather')
HDD_data %>% 
  group_by(CFG,Calendar_Qtr) %>%
  summarise(CFG_Qtr_QTY=sum(ITM_QTY)) %>% 
  ungroup() %>%
  group_by(CFG) %>%
  summarise(Avg_Qtr_HDD_QTY = sum(CFG_Qtr_QTY)/n_distinct(Calendar_Qtr)) %>% 
  mutate(HDD_Volume_Group = case_when(
    Avg_Qtr_HDD_QTY <= 100 ~ 'Small',
    Avg_Qtr_HDD_QTY > 100 & Avg_Qtr_HDD_QTY <= 1000 ~ 'Medium',
    Avg_Qtr_HDD_QTY > 1000 & Avg_Qtr_HDD_QTY <= 10000 ~ 'Large',
    Avg_Qtr_HDD_QTY > 10000 ~ 'Strategic'
  )) -> HDD_data.grouped

table(HDD_data.grouped$HDD_Volume_Group)

week_data %>% left_join(HDD_data.grouped) -> data.joined
write_excel_csv(data.joined,'~/Yao_Excels/HDD_CFG_wk.csv')
```

If I want to forecast on Clusters before CFGs, I would use Volume and Interface.

To compare my forecast results to the Lock Guidance, I will forecast for each CFG first.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(lubridate)

# Load egboost relevant libraries
library(quantmod); library(TTR); library(xgboost);

# Load timetk package
library(tidyquant)
library(timetk)
library(broom)

# Random Forest
library(party)
library(randomForest)

# Load TBATS package, as well as for ARIMA
library(forecast)
library(forecTheta) # Theta

# Load Facebook Prophet package
library(Rcpp)
library(prophet)

library(scales) # for percent

# make the code parallel using 'parallel' package
library(iterators)
library(parallel)
library(foreach)
library(doParallel)

library(seer)
 
# Calculate the number of cores
no_cores <- detectCores() - 1
registerDoParallel(no_cores)
```

## IRIS HDD Weekly Demand Forecasting for each CFG

```{r load the data}
hdd_qty <- read_feather('~/Yao_Rdata/HDD_QTY_IRIS_0731.feather')

# Weekly data For each Region
hdd_qty %>% group_by(CFG, Fiscal_Wk_End_Date,Fiscal_Wk,Calendar_Qtr) %>%
  summarise(HDD_QTY = sum(ITM_QTY)) -> HDD_Weekly
```

```{r Forecast Function}
myforecast <- function(){
  df <- data.frame(ds = temp_data$date, y = temp_data$HDD_QTY)
  n <- nrow(df)
  
  MultiQtr_Fcast <- list()
  for (i_Qtr in 1:6){
    # Split into training and test sets
    # i=1: At the beginning of FY17Q4, forecast FY18Q1 and FY17Q4 and only calculate the accuracy of FY18Q1
    Fcast_TimePoint <- filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr])$date[1]
    Fcast_EndPoint <- filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr+2])$date[1]
    train <- df %>% filter(ds < Fcast_TimePoint)
    test <- df %>% filter(ds >= Fcast_TimePoint & ds < Fcast_EndPoint)
    
    WeekAhead <- length(filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr])$date)
    forecastPeriodLen <- length(filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr+1])$date) + WeekAhead
    # print(forecastPeriodLen)
    
    Fcast_Wk_num <- 13
    if (nrow(test) < Fcast_Wk_num | nrow(train)<nrow(test)){ 
      OneQtrFcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],rep(NA,Fcast_Wk_num+1))
      MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
      next
    }
    
    Truth <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'Actual',tail(test$y,Fcast_Wk_num))
    Naive <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'naive',tail(train$y,Fcast_Wk_num))
    
    # Add time series signature # For the last date of train region
    train_augmented <- train[nrow(train),] %>%
      tk_augment_timeseries_signature()
    
    HDD.ts <- ts(train$y,frequency=365.25/7, end = train_augmented$year+train_augmented$yday/365.25)
    
    # TBATS
    fit <- tbats(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    TBATS_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'TBATS',tail(fcast$mean,Fcast_Wk_num))
    
    # ARIMA
    if (i_CFG %in% c(16,17,40,55,87)){
      ARIMA_fcast <- (c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'ARIMA',rep(NA,Fcast_Wk_num)))
    } else {
      fit <- auto.arima(HDD.ts) # Some might fail # ,D=1
      fcast <- forecast(fit,h=forecastPeriodLen)
      ARIMA_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'ARIMA',tail(fcast$mean,Fcast_Wk_num))
    }
    
    # Prophet
    m <- prophet(train)
    future <- make_future_dataframe(m, periods = forecastPeriodLen, freq = 'week',include_history = FALSE)
    fcast <- predict(m, future)
    Prophet_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'Prophet',tail(fcast$yhat,Fcast_Wk_num))
    
    # lm
    # Add time series signature
    train_augmented <- train %>%
      tk_augment_timeseries_signature()
    # maunually drop unvalid variables
    temp <-
      train_augmented %>% select(-wday.lbl,-month.lbl,-diff)
    # Remove the column with only one value
    if (nrow(temp)>1) {
      temp <- Filter(function(x)(length(unique(x))>1), temp)
    }
    
    # Model using the augmented features
    fit <- lm(y ~ ., data = na.omit(temp)) # Will need to try using less number of features! Avoid overfitting.
    # We need to again augment the time series signature to the test set.
    test_augmented <- test %>%
      tk_augment_timeseries_signature()
    yhat_test <- predict(fit, newdata = test_augmented)
    lm_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'lm',tail(yhat_test,Fcast_Wk_num))
    
    # Random Forest
    # Model using the augmented features
    fit <- randomForest(y ~ ., data = na.omit(temp))
    yhat_test <- predict(fit, newdata = test_augmented)
    RF_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'RF',tail(yhat_test,Fcast_Wk_num))
    
    # Xgboost
    # Train the xgboost model using the "xgboost" function
    dtrain = xgb.DMatrix(data = as.matrix(select(temp,-y,-ds)), label = temp$y)
    xgModel = xgboost(data = dtrain, nrounds = ceiling(nrow(dtrain)/3)) # sqrt(nrow(dtrain)) for classification, n/3 for regression
    # Make the predictions on the test data
    temp_test <- test_augmented[,colnames(temp)]
    preds = predict(xgModel, as.matrix(select(temp_test,-y,-ds)))
    Xgboost_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'Xgboost',tail(preds,Fcast_Wk_num))
    
    # ets
    fit <- ets(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ets_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'ets',tail(fcast$mean,Fcast_Wk_num))
   
    # rw: random walk
    rw_fit <- rwf(HDD.ts,drift=FALSE, h=forecastPeriodLen)
    forecastRW <- forecast(rw_fit)$mean
    rw_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'rw',tail(forecastRW,Fcast_Wk_num))
    
    # rwd: random walk with drift
    rw_fit <- rwf(HDD.ts,drift=TRUE, h=forecastPeriodLen)
    forecastRWD <- forecast(rw_fit)$mean
    rwd_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'rwd',tail(forecastRWD,Fcast_Wk_num))
    
    # wn: white noise # Calculate accuracy measure based on white noise process
    fit_WN <- auto.arima(HDD.ts, d=0, D=0, max.p=0, max.q = 0,
                     max.Q=0, max.P = 0)
    forecastWN <- forecast(fit_WN,h=forecastPeriodLen)$mean
    wn_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'wn',tail(forecastWN,Fcast_Wk_num))
    
    # theta: standard theta method
    # if (i_Qtr>2) {
    #   forecastTheta <- stheta(HDD.ts,h=forecastPeriodLen, s='additive')$mean
    #   theta_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'theta',tail(forecastTheta,Fcast_Wk_num))
    # }
    # Error in decompose(y, type = s_type) : 
    #   time series has no or less than 2 periods

    # stlar
    forecastSTLAR <- stlar(HDD.ts,h=forecastPeriodLen)$mean
    stlar_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'stlar',tail(forecastSTLAR,Fcast_Wk_num))
    
    # nn
    fit_nnetar <- nnetar(HDD.ts)
    forecastnnetar <- forecast(fit_nnetar, h=forecastPeriodLen)$mean
    nn_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'nn',tail(forecastnnetar,Fcast_Wk_num))

    # snaive
    # forecastSNAIVE <- snaive(HDD.ts, h=forecastPeriodLen)$mean
    
    # mstlarima # This does not work.
    # fit_stlf <- stlf(HDD.ts, method=mtd)
    # forecastMSTL <- forecast(fit_stlf, h=forecastPeriodLen)$mean
    
    # mstlets
    
    OneQtrFcast <- rbind(Truth,Naive,TBATS_fcast,ARIMA_fcast,Prophet_fcast,lm_fcast,RF_fcast,Xgboost_fcast,ets_fcast,rw_fcast,rwd_fcast,wn_fcast,stlar_fcast,nn_fcast)
    MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
  }
  return(MultiQtr_Fcast)
}
```


```{r Run the fuctions parallelly}
HDD_Weekly$date <- ymd(HDD_Weekly$Fiscal_Wk_End_Date)
CFGgroups <- levels(factor(HDD_Weekly$CFG))
len <- length(CFGgroups)

Fcast_Calendar_Qtr <- c("CY16CQ4","CY17CQ1","CY17CQ2","CY17CQ3","CY17CQ4","CY18CQ1","CY18CQ2","CY18CQ3") # This is the time point where you forecast.

# Use i_CFG=4 for test
Comb_fcast <- foreach(i_CFG=1:len, .combine=rbind,  .packages=c('tidyverse','forecast','forecTheta','seer','Rcpp','ggthemes','lubridate','tidyquant','timetk','prophet','randomForest','party','quantmod','TTR','xgboost')) %dopar% {
  # Forecast for each CFG
  temp_data <- HDD_Weekly %>% filter(CFG==CFGgroups[i_CFG])
  oneCFG_fcast <- myforecast()
  oneCFG_fcast
}

# Assign Colume names
weekNames <- c(sprintf("W0%d", 1:9),sprintf("W%d", 10:13))
dimnames(Comb_fcast)[[2]] <- c('CFG','Quarter','Model',as.character(weekNames))
# HDD quntatity should be numeric.
Comb_fcast[,4:ncol(Comb_fcast)] <- lapply(Comb_fcast[,4:ncol(Comb_fcast)], function(x) as.numeric(as.character(x)))
Comb_fcast[,1:3] <- lapply(Comb_fcast[,1:3], function(x) as.character(x))
# Transformation
Comb_fcast %>% data.frame() %>% gather(key='Fiscal_Wk',value='HDD_QTY',W01:W13) -> All_fcast

All_fcast[,1:4] <- lapply(All_fcast[,1:4], function(x) as.character(x))
All_fcast$HDD_QTY <- as.numeric(as.character(All_fcast$HDD_QTY))

# For plot, do not delete the NA for now.
# saveRDS(All_fcast, 'All_fcast_CV.rds') # my_data <- readRDS(file)
write_feather(All_fcast,"~/Yao_Rdata/All_fcast_CV_Cal_Qtr_0807_v2.feather")
write_excel_csv(All_fcast,"~/Yao_Excels/All_fcast_CV_Cal_Qtr_0807_v2.csv")
```

```{r Error Calculation}
yao_hdd = read_csv('~/Yao_Excels/All_fcast_CV_Cal_Qtr_0807_v2.csv', na = 'NA')

result_q2_yao = yao_hdd %>% 
  filter(Quarter == 'CY18CQ2') %>% 
  group_by(CFG, Model) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter == 'CY18CQ2') %>% 
      group_by(CFG, Model) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual'), by = 'CFG') %>% 
  mutate(APE = abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = HDD_Actual/Yao_QTY) 

result_q2_yao  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE))

# Commodity "Accuracy"
result_q2_yao %>%
  select(CFG,Model,AttainmentRate) %>%
  filter(AttainmentRate>=0.8 & AttainmentRate<=1.2) %>%
  spread(Model,AttainmentRate) %>%
  ungroup() %>%
  select(-CFG) %>%
  lapply(function(x) length(which(!is.na(x)))/length(x)) %>%
  lapply(function(x)  as.numeric(as.character(x))) %>% data.frame() -> CommodityAccuracy

# Compare to Lock Guidance
yao_hdd %>% 
  filter(Quarter == 'CY18CQ2') %>% 
  group_by(CFG, Model) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter == 'CY18CQ2') %>% 
      group_by(CFG, Model) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual'), by = 'CFG') %>%
  spread(Model,Yao_QTY) -> Yao_HDD_QTY

Lock_Guidance_CY18CQ2_Results %>% 
  select(CFG,Lock_Forecast,Actual) %>%
  left_join(Yao_HDD_QTY, by = 'CFG') %>%
  gather(Model,HDD_QTY,-CFG,-Actual,-HDD_Actual) %>%
  filter(!is.na(Actual) & !is.na(HDD_Actual)) %>% # only keep the common CFGs
  mutate(APE = abs(HDD_Actual - HDD_QTY) / HDD_Actual,
         AttainmentRate = HDD_Actual/HDD_QTY) -> result_q2

result_q2  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE)) -> MAPE.results

# Commodity "Accuracy"
result_q2 %>%
  select(CFG,Model,AttainmentRate) %>%
  filter(AttainmentRate>=0.8 & AttainmentRate<=1.2) %>%
  spread(Model,AttainmentRate) %>%
  ungroup() %>%
  select(-CFG) %>%
  lapply(function(x) length(which(!is.na(x)))/length(x)) %>%
  lapply(function(x)  as.numeric(as.character(x))) %>% data.frame() %>%
  gather(Model,Accuracy) %>%
  left_join(MAPE.results) -> EvalResults
  
write_excel_csv(EvalResults,"~/Yao_Excels/EvalResults_v2.csv")

# Analyze the errors
result_q2 %>% 
  left_join(select(data.joined,CFG,CFG_GROUP,Interface,Interface_w_Speed,Capacity,Drive_Form_Factor,Drive_Entrypted_Code,HDD_RPM,HDD_Data_Sector_Format,Capacity_Num,Avg_Qtr_HDD_QTY,HDD_Volume_Group) %>% unique() %>% arrange(CFG), by = 'CFG') -> Error.w.Attributes

write_excel_csv(Error.w.Attributes,"~/Yao_Excels/Error_W_Attributes_v2.csv")
```


```{r close the clusters}
# for Parallel package
stopImplicitCluster()
```

