---
title: "IRIS HDD Forecasting for each CFG by Region with Cross-Validation"
author: "Yao"
date: "June 27-July 3, 2018"
output: html_document
---

This is the parallel function version using weekly data and different models, including TBATS, Prophet, ARIMA, lm, and Random Forest, to forecast one quarter HDD demand with one quarter ahead and with cross-validation.
Updated on 7/3/2018. Do not use FY19Q1 to train models.
Updated on 7/26/2018. Add more models!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
options(scipen = 999)
library(lubridate)

# Load egboost relevant libraries
library(quantmod); library(TTR); library(xgboost);

# Load timetk package
library(tidyquant)
library(timetk)
library(broom)

# Random Forest
library(party)
library(randomForest)

# Load TBATS package, as well as for ARIMA
library(forecast)
library(forecTheta) # Theta

# Load Facebook Prophet package
library(Rcpp)
library(prophet)

library(scales) # for percent

# make the code parallel using 'parallel' package
library(iterators)
library(parallel)
library(foreach)
library(doParallel)

library(feather)
library(seer)
 
# Calculate the number of cores
no_cores <- detectCores() - 1
registerDoParallel(no_cores)
```

## IRIS HDD Weekly Demand Forecasting for each CFG

```{r load the data}
hdd_qty <- read_feather("~/Yao_Rdata/HDD_QTY_IRIS.feather")

# Weekly data For each Region
hdd_qty %>% group_by(CFG, RGN_DESC, Fiscal_Wk_End_Date,Fiscal_Wk,Fiscal_Qtr) %>%
  summarise(HDD_QTY = sum(PART_QTY)) -> HDD_Weekly
# Weekly data Globally
hdd_qty %>% group_by(CFG, Fiscal_Wk_End_Date,Fiscal_Wk,Fiscal_Qtr) %>%
  summarise(HDD_QTY = sum(PART_QTY)) -> HDD_Weekly_Global
HDD_Weekly_Global$RGN_DESC <- 'Global'
HDD_Weekly_Global %>% full_join(HDD_Weekly) -> HDD_Weekly
```

```{r Forecast Function}
myforecast <- function(){
  df <- data.frame(ds = temp_data$date, y = temp_data$HDD_QTY)
  n <- nrow(df)
  
  MultiQtr_Fcast <- list()
  for (i_Qtr in 1:4){
    # Split into training and test sets
    # i=1: At the beginning of FY17Q4, forecast FY18Q1 and FY17Q4 and only calculate the accuracy of FY18Q1
    Fcast_TimePoint <- filter(temp_data,Fiscal_Qtr==Fcast_Qtr[i_Qtr])$date[1]
    Fcast_EndPoint <- filter(temp_data,Fiscal_Qtr==Fcast_Qtr[i_Qtr+2])$date[1]
    train <- df %>% filter(ds < Fcast_TimePoint)
    test <- df %>% filter(ds >= Fcast_TimePoint & ds < Fcast_EndPoint)
    
    WeekAhead <- length(filter(temp_data,Fiscal_Qtr==Fcast_Qtr[i_Qtr])$date)
    forecastPeriodLen <- length(filter(temp_data,Fiscal_Qtr==Fcast_Qtr[i_Qtr+1])$date) + WeekAhead
    # print(forecastPeriodLen)
    
    Fcast_Wk_num <- 13
    if (nrow(test) < Fcast_Wk_num | nrow(train)<nrow(test)){ 
      OneQtrFcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],rep(NA,Fcast_Wk_num+1))
      MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
      next
    }
    
    Truth <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'HDD_QTY',tail(test$y,Fcast_Wk_num))
    
    # Add time series signature # For the last date of train region
    train_augmented <- train[nrow(train),] %>%
      tk_augment_timeseries_signature()
    
    HDD.ts <- ts(train$y,frequency=365.25/7, end = train_augmented$year+train_augmented$yday/365.25)
    
    # TBATS
    fit <- tbats(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    TBATS_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'TBATS',tail(fcast$mean,Fcast_Wk_num))
    
    # ARIMA
    if (i_CFG %in% c(16,17,40,55,87) & i_Region==3){
      ARIMA_fcast <- (c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'ARIMA',rep(NA,Fcast_Wk_num)))
    } else {
      fit <- auto.arima(HDD.ts) # Some might fail # ,D=1
      fcast <- forecast(fit,h=forecastPeriodLen)
      ARIMA_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'ARIMA',tail(fcast$mean,Fcast_Wk_num))
    }
    
    # Prophet
    m <- prophet(train)
    future <- make_future_dataframe(m, periods = forecastPeriodLen, freq = 'week',include_history = FALSE)
    fcast <- predict(m, future)
    Prophet_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'Prophet',tail(fcast$yhat,Fcast_Wk_num))
    
    # lm
    # Add time series signature
    train_augmented <- train %>%
      tk_augment_timeseries_signature()
    # maunually drop unvalid variables
    temp <-
      train_augmented %>% select(-wday.lbl,-month.lbl,-diff)
    # Remove the column with only one value
    if (nrow(temp)>1) {
      temp <- Filter(function(x)(length(unique(x))>1), temp)
    }
    
    # Model using the augmented features
    fit <- lm(y ~ ., data = na.omit(temp)) # Will need to try using less number of features! Avoid overfitting.
    # We need to again augment the time series signature to the test set.
    test_augmented <- test %>%
      tk_augment_timeseries_signature()
    yhat_test <- predict(fit, newdata = test_augmented)
    lm_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'lm',tail(yhat_test,Fcast_Wk_num))
    
    # Random Forest
    # Model using the augmented features
    fit <- randomForest(y ~ ., data = na.omit(temp))
    yhat_test <- predict(fit, newdata = test_augmented)
    RF_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'RF',tail(yhat_test,Fcast_Wk_num))
    
    # Xgboost
    # Train the xgboost model using the "xgboost" function
    dtrain = xgb.DMatrix(data = as.matrix(select(temp,-y,-ds)), label = temp$y)
    xgModel = xgboost(data = dtrain, nrounds = ceiling(sqrt(nrow(dtrain)))) # sqrt(nrow(dtrain)), I guess
    # Make the predictions on the test data
    temp_test <- test_augmented[,colnames(temp)]
    preds = predict(xgModel, as.matrix(select(temp_test,-y,-ds)))
    Xgboost_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'Xgboost',tail(preds,Fcast_Wk_num))
    
    # ets
    fit <- ets(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ets_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'ets',tail(fcast$mean,Fcast_Wk_num))
   
    # rw: random walk
    rw_fit <- rwf(HDD.ts,drift=FALSE, h=forecastPeriodLen)
    forecastRW <- forecast(rw_fit)$mean
    rw_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'rw',tail(forecastRW,Fcast_Wk_num))
    
    # rwd: random walk with drift
    rw_fit <- rwf(HDD.ts,drift=TRUE, h=forecastPeriodLen)
    forecastRWD <- forecast(rw_fit)$mean
    rwd_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'rwd',tail(forecastRWD,Fcast_Wk_num))
    
    # wn: white noise # Calculate accuracy measure based on white noise process
    fit_WN <- auto.arima(HDD.ts, d=0, D=0, max.p=0, max.q = 0,
                     max.Q=0, max.P = 0)
    forecastWN <- forecast(fit_WN,h=forecastPeriodLen)$mean
    wn_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'wn',tail(forecastWN,Fcast_Wk_num))
    
    # theta: standard theta method
    # if (i_Qtr>2) {
    #   forecastTheta <- stheta(HDD.ts,h=forecastPeriodLen, s='additive')$mean
    #   theta_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'theta',tail(forecastTheta,Fcast_Wk_num))
    # }
    # Error in decompose(y, type = s_type) : 
    #   time series has no or less than 2 periods

    # stlar
    forecastSTLAR <- stlar(HDD.ts,h=forecastPeriodLen)$mean
    stlar_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'stlar',tail(forecastSTLAR,Fcast_Wk_num))
    
    # nn
    fit_nnetar <- nnetar(HDD.ts)
    forecastnnetar <- forecast(fit_nnetar, h=forecastPeriodLen)$mean
    nn_fcast <- c(CFGgroups[i_CFG],Regions[i_Region],Fcast_Qtr[i_Qtr+1],'nn',tail(forecastnnetar,Fcast_Wk_num))

    # snaive
    # forecastSNAIVE <- snaive(HDD.ts, h=forecastPeriodLen)$mean
    
    # mstlarima # This does not work.
    # fit_stlf <- stlf(HDD.ts, method=mtd)
    # forecastMSTL <- forecast(fit_stlf, h=forecastPeriodLen)$mean
    
    # mstlets
    
    OneQtrFcast <- rbind(Truth,TBATS_fcast,ARIMA_fcast,Prophet_fcast,lm_fcast,RF_fcast,Xgboost_fcast,ets_fcast,rw_fcast,rwd_fcast,wn_fcast,stlar_fcast,nn_fcast)
    MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
  }
  return(MultiQtr_Fcast)
}
```


```{r Run the fuctions parallelly}
HDD_Weekly$date <- ymd(HDD_Weekly$Fiscal_Wk_End_Date)
CFGgroups <- levels(factor(HDD_Weekly$CFG))
len <- length(CFGgroups)

Regions <- levels(factor(HDD_Weekly$RGN_DESC))
num.Regions <- length(Regions)
Fcast_Qtr <- c("FY17Q4","FY18Q1","FY18Q2","FY18Q3","FY18Q4","FY19Q1","FY19Q2") # This is the time point where you forecast.

Comb_fcast <- data.frame()
for (i_Region in 1:num.Regions){
  Comb_fcast0 <- foreach(i_CFG=1:len, .combine=rbind, .packages=c('tidyverse','forecast','forecTheta','seer','Rcpp','ggthemes','lubridate','tidyquant','timetk','prophet','randomForest','party','quantmod','TTR','xgboost')) %dopar% {
    # Forecast for each CFG
    temp_data <- HDD_Weekly %>% filter(CFG==CFGgroups[i_CFG],RGN_DESC==Regions[i_Region])
    oneCFG_fcast <- myforecast()
    oneCFG_fcast
  }
  Comb_fcast <- rbind(Comb_fcast,Comb_fcast0)
}

# Assign Colume names
weekNames <- c(sprintf("W0%d", 1:9),sprintf("W%d", 10:13))
dimnames(Comb_fcast)[[2]] <- c('CFG','Region','Quarter','Model',as.character(weekNames))
# HDD quntatity should be numeric.
Comb_fcast[,5:ncol(Comb_fcast)] <- lapply(Comb_fcast[,5:ncol(Comb_fcast)], function(x) as.numeric(as.character(x)))
Comb_fcast[,1:4] <- lapply(Comb_fcast[,1:4], function(x) as.character(x))
# Transformation
Comb_fcast %>% gather(key='Fiscal_Wk',value='HDD_QTY',W01:W13) -> All_fcast

# For plot, do not delete the NA for now.
# saveRDS(All_fcast, 'All_fcast_CV.rds') # my_data <- readRDS(file)
write_feather(All_fcast,"~/Yao_Rdata/All_fcast_CV.feather")
```

```{r close the clusters}
# for Parallel package
stopImplicitCluster()
```
