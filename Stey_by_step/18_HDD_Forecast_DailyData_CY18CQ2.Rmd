---
title: "HDD Forecast CY18CQ2 uisng Daily Data"
author: "Yao"
date: "August 13, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(feather)
options(scipen = 999)
library(lubridate)

# Load egboost relevant libraries
library(quantmod); library(TTR); library(xgboost);

# Load timetk package
library(tidyquant)
library(timetk)
library(broom)

# Random Forest
library(party)
library(randomForest)

# Load TBATS package, as well as for ARIMA
library(forecast)
library(forecTheta) # Theta

# Load Facebook Prophet package
library(Rcpp)
library(prophet)

library(scales) # for percent

# make the code parallel using 'parallel' package
library(iterators)
library(parallel)
library(foreach)
library(doParallel)

library(seer)
 
# Calculate the number of cores
no_cores <- detectCores() - 1
registerDoParallel(no_cores)
```

Got Lock Guidance CY18CQ2 Results from Cinkie.

```{r Read the Loack Guidance CY18CQ2 Results}
library(readr)
Lock_Guidance_CY18CQ2_Results <- read_csv("~/Yao_Excels/Lock Guidance CY18CQ2 Results.csv")
```


Using Daily Data to forecast.

## IRIS HDD Daily Demand Forecasting for each CFG

```{r load the data}

hdd_qty <- read_csv('~/Yao_Excels/ESG HDD Daily Volume.csv')

# Daily data For each Region
hdd_qty %>% 
  filter(ESI_Flag=="N" & ITM_TYPE=="HDD") %>%
  group_by(CFG, Fiscal_Date,Calendar_Qtr) %>%
  summarise(HDD_QTY = sum(ITM_QTY)) -> HDD_Daily

HDD_Daily %>% 
  filter(Calendar_Qtr=='CY18CQ2') %>%
  select(CFG,Calendar_Qtr,HDD_QTY) %>%
  ungroup() %>%
  group_by(CFG,Calendar_Qtr) %>%
  summarise(Cal_Qtr_HDD = sum(HDD_QTY)) %>%
  left_join(Lock_Guidance_CY18CQ2_Results %>% 
                           select(CFG,Calendar_Qtr,Lock_Forecast,Actual)) %>%
  na.omit() -> df.combined
```

The Actual from Hyperiod is different from Cinkie's data?? Ask her later. Yes, it is different and we should trust the newly downloaded data.

```{r Forecast Function}
myforecast <- function(){
  df <- data.frame(ds = temp_data$date, y = temp_data$HDD_QTY)
  n <- nrow(df)
  
  MultiQtr_Fcast <- list()
  for (i_Qtr in 1:6){
    # Split into training and test sets
    # i=1: At the beginning of FY17Q4, forecast FY18Q1 and FY17Q4 and only calculate the accuracy of FY18Q1
    Fcast_TimePoint <- filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr])$date[1]
    Fcast_EndPoint <- filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr+2])$date[1]
    train <- df %>% filter(ds < Fcast_TimePoint)
    test <- df %>% filter(ds >= Fcast_TimePoint & ds < Fcast_EndPoint)
    
    DayAhead <- length(filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr])$date)
    forecastPeriodLen <- length(filter(temp_data,Calendar_Qtr==Fcast_Calendar_Qtr[i_Qtr+1])$date) + DayAhead
    # print(forecastPeriodLen)
    
    Fcast_Day_num <- 13*7
    if (nrow(test) < Fcast_Day_num | nrow(train)<3){ 
      OneQtrFcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],rep(NA,Fcast_Day_num+1))
      MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
      next
    }
    
    Truth <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'Actual',tail(test$y,Fcast_Day_num))
    Naive <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'naive',tail(train$y,Fcast_Day_num))
    
    # Add time series signature # For the last date of train region
    train_augmented <- train[nrow(train),] %>%
      tk_augment_timeseries_signature()
    
    HDD.ts <- ts(train$y,frequency=365.25/7, end = train_augmented$year+train_augmented$yday/365.25)
    
    # TBATS
    fit <- tbats(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    TBATS_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'TBATS',tail(fcast$mean,Fcast_Day_num))
    
    # ARIMA
    if (i_CFG %in% c(16,17,40,55,87)){
      ARIMA_fcast <- (c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'ARIMA',rep(NA,Fcast_Day_num)))
    } else {
      fit <- auto.arima(HDD.ts) # Some might fail # ,D=1
      fcast <- forecast(fit,h=forecastPeriodLen)
      ARIMA_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'ARIMA',tail(fcast$mean,Fcast_Day_num))
    }
    
    # Prophet
    m <- prophet(train)
    future <- make_future_dataframe(m, periods = forecastPeriodLen, freq = 'week',include_history = FALSE)
    fcast <- predict(m, future)
    Prophet_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'Prophet',tail(fcast$yhat,Fcast_Day_num))
    
    # lm
    # Add time series signature
    train_augmented <- train %>%
      tk_augment_timeseries_signature()
    # maunually drop unvalid variables
    temp <-
      train_augmented %>% select(-wday.lbl,-month.lbl,-diff)
    # Remove the column with only one value
    if (nrow(temp)>1) {
      temp <- Filter(function(x)(length(unique(x))>1), temp)
    }
    
    # Model using the augmented features
    fit <- lm(y ~ ., data = na.omit(temp)) # Will need to try using less number of features! Avoid overfitting.
    # We need to again augment the time series signature to the test set.
    test_augmented <- test %>%
      tk_augment_timeseries_signature()
    yhat_test <- predict(fit, newdata = test_augmented)
    lm_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'lm',tail(yhat_test,Fcast_Day_num))
    
    # Random Forest
    # Model using the augmented features
    fit <- randomForest(y ~ ., data = na.omit(temp))
    yhat_test <- predict(fit, newdata = test_augmented)
    RF_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'RF',tail(yhat_test,Fcast_Day_num))
    
    # Xgboost
    # Train the xgboost model using the "xgboost" function
    dtrain = xgb.DMatrix(data = as.matrix(select(temp,-y,-ds)), label = temp$y)
    xgModel = xgboost(data = dtrain, nrounds = ceiling(nrow(dtrain)/3)) # sqrt(nrow(dtrain)) for classification, n/3 for regression
    # Make the predictions on the test data
    temp_test <- test_augmented[,colnames(temp)]
    preds = predict(xgModel, as.matrix(select(temp_test,-y,-ds)))
    Xgboost_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'Xgboost',tail(preds,Fcast_Day_num))
    
    # ets
    fit <- ets(HDD.ts)
    fcast <- forecast(fit,h=forecastPeriodLen)
    ets_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'ets',tail(fcast$mean,Fcast_Day_num))
   
    # rw: random walk
    rw_fit <- rwf(HDD.ts,drift=FALSE, h=forecastPeriodLen)
    forecastRW <- forecast(rw_fit)$mean
    rw_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'rw',tail(forecastRW,Fcast_Day_num))
    
    # rwd: random walk with drift
    rw_fit <- rwf(HDD.ts,drift=TRUE, h=forecastPeriodLen)
    forecastRWD <- forecast(rw_fit)$mean
    rwd_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'rwd',tail(forecastRWD,Fcast_Day_num))
    
    # wn: white noise # Calculate accuracy measure based on white noise process
    fit_WN <- auto.arima(HDD.ts, d=0, D=0, max.p=0, max.q = 0,
                     max.Q=0, max.P = 0)
    forecastWN <- forecast(fit_WN,h=forecastPeriodLen)$mean
    wn_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'wn',tail(forecastWN,Fcast_Day_num))
    
    # theta: standard theta method
    # if (i_Qtr>2) {
    #   forecastTheta <- stheta(HDD.ts,h=forecastPeriodLen, s='additive')$mean
    #   theta_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'theta',tail(forecastTheta,Fcast_Day_num))
    # }
    # Error in decompose(y, type = s_type) : 
    #   time series has no or less than 2 periods

    # stlar
    # forecastSTLAR <- stlar(HDD.ts,h=forecastPeriodLen)$mean
    # stlar_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'stlar',tail(forecastSTLAR,Fcast_Day_num))
    
    # nn
    # fit_nnetar <- nnetar(HDD.ts)
    # forecastnnetar <- forecast(fit_nnetar, h=forecastPeriodLen)$mean
    # nn_fcast <- c(CFGgroups[i_CFG],Fcast_Calendar_Qtr[i_Qtr+1],'nn',tail(forecastnnetar,Fcast_Day_num))

    # snaive
    # forecastSNAIVE <- snaive(HDD.ts, h=forecastPeriodLen)$mean
    
    # mstlarima # This does not work.
    # fit_stlf <- stlf(HDD.ts, method=mtd)
    # forecastMSTL <- forecast(fit_stlf, h=forecastPeriodLen)$mean
    
    # mstlets
    
    OneQtrFcast <- rbind(Truth,Naive,TBATS_fcast,ARIMA_fcast,Prophet_fcast,lm_fcast,RF_fcast,Xgboost_fcast,ets_fcast,rw_fcast,rwd_fcast,wn_fcast) #,stlar_fcast,nn_fcast
    MultiQtr_Fcast <- rbind(MultiQtr_Fcast,OneQtrFcast)
  }
  return(MultiQtr_Fcast)
}
```


```{r Run the fuctions parallelly}
HDD_Daily$date <- ymd(HDD_Daily$Fiscal_Date)
CFGgroups <- levels(factor(HDD_Daily$CFG)) 
CFGgroups <- CFGgroups[CFGgroups %in% df.combined$CFG]
len <- length(CFGgroups)

Fcast_Calendar_Qtr <- c("CY16CQ4","CY17CQ1","CY17CQ2","CY17CQ3","CY17CQ4","CY18CQ1","CY18CQ2","CY18CQ3") # This is the time point where you forecast.

# Use i_CFG=4 for test
Comb_fcast <- foreach(i_CFG=1:len, .combine=rbind,  .packages=c('tidyverse','forecast','forecTheta','seer','Rcpp','ggthemes','lubridate','tidyquant','timetk','prophet','randomForest','party','quantmod','TTR','xgboost')) %dopar% {
  # Forecast for each CFG
  temp_data <- HDD_Daily %>% filter(CFG==CFGgroups[i_CFG])
  oneCFG_fcast <- myforecast()
  oneCFG_fcast
}

# Assign Colume names
DayNames <- c(sprintf("D0%d", 1:9),sprintf("D%d", 10:91))
dimnames(Comb_fcast)[[2]] <- c('CFG','Quarter','Model',as.character(DayNames))
# HDD quntatity should be numeric.
Comb_fcast[,4:ncol(Comb_fcast)] <- lapply(Comb_fcast[,4:ncol(Comb_fcast)], function(x) as.numeric(as.character(x)))
Comb_fcast[,1:3] <- lapply(Comb_fcast[,1:3], function(x) as.character(x))
# Transformation
Comb_fcast %>% data.frame() %>% gather(key='Day',value='HDD_QTY',D01:D91) -> All_fcast

All_fcast[,1:4] <- lapply(All_fcast[,1:4], function(x) as.character(x))
All_fcast$HDD_QTY <- as.numeric(as.character(All_fcast$HDD_QTY))

# For plot, do not delete the NA for now.
# saveRDS(All_fcast, 'All_fcast_CV.rds') # my_data <- readRDS(file)
write_feather(All_fcast,"~/Yao_Rdata/All_fcast_CV_Cal_Qtr_Daily.feather")
write_excel_csv(All_fcast,"~/Yao_Excels/All_fcast_CV_Cal_Qtr_Daily.csv")
```

```{r Error Calculation}
yao_hdd = read_csv('~/Yao_Excels/All_fcast_CV_Cal_Qtr_Daily.csv', na = 'NA')

result_4q_yao = yao_hdd %>% 
  filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
  group_by(CFG, Model,Quarter) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  ungroup() %>%
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter %in% c('CY17CQ3','CY17CQ4','CY18CQ1','CY18CQ2')) %>% 
      group_by(CFG, Model,Quarter) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual')) %>% 
  mutate(APE = abs(HDD_Actual - Yao_QTY) / HDD_Actual,
         AttainmentRate = HDD_Actual/Yao_QTY) 

result_4q_yao  %>%
  group_by(Model) %>% 
    summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) -> result_4q

# Compare to Lock Guidance and only consider CY18CQ2
yao_hdd %>% 
  filter(Quarter == 'CY18CQ2') %>% 
  group_by(CFG, Model) %>% 
  summarise(Yao_QTY = sum(HDD_QTY)) %>% 
  filter(!is.na(Yao_QTY) & Model == 'Actual') %>% 
  select(-Model) %>% 
  rename(HDD_Actual = Yao_QTY) %>% 
  left_join(
    yao_hdd %>% 
      filter(Quarter == 'CY18CQ2') %>% 
      group_by(CFG, Model) %>% 
      summarise(Yao_QTY = sum(HDD_QTY)) %>% 
      filter(!is.na(Yao_QTY) & Model != 'Actual'), by = 'CFG') %>%
  spread(Model,Yao_QTY) -> Yao_HDD_QTY

Lock_Guidance_CY18CQ2_Results %>% 
  select(CFG,Lock_Forecast) %>%
  left_join(Yao_HDD_QTY, by = 'CFG') %>%
  gather(Model,HDD_QTY,-CFG,-HDD_Actual) %>%
  filter(!is.na(HDD_Actual)) %>% # only keep the common CFGs
  mutate(APE = abs(HDD_Actual - HDD_QTY) / HDD_Actual,
         AttainmentRate = HDD_Actual/HDD_QTY) -> result_q2

result_q2  %>%
  group_by(Model) %>% 
  summarise(MAPE = mean(APE, na.rm = TRUE),weighted.MAPE = weighted.mean(APE,HDD_Actual, na.rm = TRUE)) -> MAPE.results

# Commodity "Accuracy"
result_q2 %>%
  select(CFG,Model,AttainmentRate,HDD_Actual) %>%
  mutate(Attain_Flag = case_when(
    AttainmentRate >= 0.8 & AttainmentRate <= 1.2 ~ 'Green',
    AttainmentRate < 0.8 ~ 'Blue',
    AttainmentRate > 1.2 ~ 'Red')) %>%
  group_by(Model) %>%
    summarise(Accuracy=length(which(Attain_Flag=="Green"))/length(Attain_Flag),weighted.Accuracy=sum(HDD_Actual[which(Attain_Flag=="Green")])/sum(HDD_Actual)) %>%
  left_join(MAPE.results) -> EvalResults
  
write_excel_csv(EvalResults,"~/Yao_Excels/EvalResults_Daily.csv")
```


```{r close the clusters}
# for Parallel package
stopImplicitCluster()
```

