---
title: "IRIS HDD Forecasting for each CFG using Weekly Data"
author: "Yao"
date: "June 6-8, 2018"
output: html_document
---

This is the parallel function version using Facebook 'Prophet'.
It turned out that linear models work better.
6/11/2018 Forecast F19W07-W15 and then compare my forecasting results to MRP.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
options(scipen = 999)
library(lubridate)
library(tidyquant)

# Load Facebook Prophet package
library(Rcpp)
library(prophet)

library(scales) # for percent

# make the code parallel using 'parallel' package
library(iterators)
library(parallel)
library(foreach)
library(doParallel)
 
# Calculate the number of cores
no_cores <- detectCores() - 1
registerDoParallel(no_cores)
```

## IRIS HDD Weekly Demand Forecasting for each CFG

```{r load the data}
load("HDD_QTY_IRIS.RData")

# Weekly data
hdd_qty %>% group_by(CFG, Fiscal_Wk_End_Date) %>%
  summarise(HDD_QTY = sum(PART_QTY)) -> HDD_Weekly
```

```{r Cross Validation Function}
mycrossvalidation <- function(i,df,k,forecastPeriodLen){

  # Split into training and test sets
  train <- df %>% filter(ds < df$ds[k+i])
  test <- df %>% filter(ds >= df$ds[k+i] & ds <= df$ds[k+i+forecastPeriodLen-1])
  
  m <- prophet(train) # growth = 'logistic' works bad..
  
  future <- make_future_dataframe(m, periods = forecastPeriodLen, freq = 'week',include_history = FALSE)
  
  fcast <- predict(m, future)

  pred_test <- test %>%
    add_column(yhat = fcast$yhat) %>%
    mutate(.resid = fcast$yhat - y)
  
  if (1){
    annotatePos <- max(df$y)*0.85
    rectPos <- max(df$y)
    p <- ggplot(aes(x = ds), data = df) +
      labs(title = paste(CFGgroups[i_CFG],"HDD Weekly Demand"), x = "Fiscal Week End Date",y='Weekly Demand') +
      #theme_tq() +
      annotate("text", x = ymd(df$ds[k+i-k/2]), y = annotatePos,
               color = palette_light()[[1]], label = "Train Region") +
      annotate("text", x = ymd(df$ds[k+i+forecastPeriodLen/2]), y = annotatePos*0.85,
               color = palette_light()[[1]], label = "Test Region") +
      geom_rect(xmin = as.numeric(ymd(df$ds[k+i])),
                xmax = as.numeric(ymd(df$ds[k+i+forecastPeriodLen-1])),
                ymin = 0, ymax = rectPos,
                fill = palette_light()[[4]], alpha = 0.01) + 
      geom_rect(xmin = as.numeric(ymd(df$ds[1])),
                xmax = as.numeric(ymd(df$ds[k+i])),
                ymin = 0, ymax = rectPos,
                fill = palette_light()[[3]], alpha = 0.01) + 
      expand_limits(y = 0) +
      geom_point(aes(x = ds, y = y), data = train, alpha = 0.5, color = palette_light()[[1]]) + 
      geom_line(aes(x = ds, y = y), data = train, alpha = 0.5, color = palette_light()[[1]]) +
      geom_point(aes(x = ds, y = y), data = pred_test, alpha = 0.5, color = palette_light()[[1]]) + 
      geom_line(aes(x = ds, y = y), data = pred_test, alpha = 0.5, color = palette_light()[[1]]) +
      geom_point(aes(x = ds, y = yhat), data = pred_test, alpha = 0.5, color = palette_light()[[2]]) + 
      geom_line(aes(x = ds, y = yhat), data = pred_test, alpha = 0.5, color = palette_light()[[2]]) +
      theme_tq() 
    #print(p)
    ggsave(filename = paste(CFGgroups[i_CFG],df$ds[k+i],".png"), p, width=10)
  }

  # Weekly Error Calculation
  temp <- pred_test %>%
    mutate(pct_err = .resid/y * 100, ape = abs(pct_err))
  mape_fcastPeriod_values <- mean(temp$ape[forecastPeriodLen/2:forecastPeriodLen])
  
  # Quarterly Error Calculation
  MAPE_Quarterly_fcastPeriod <- abs(sum(temp$yhat[forecastPeriodLen/2:forecastPeriodLen])-sum(temp$y[forecastPeriodLen/2:forecastPeriodLen]))/sum(temp$y[forecastPeriodLen/2:forecastPeriodLen]) * 100

  return(c(mape_fcastPeriod_values,MAPE_Quarterly_fcastPeriod))
}
```


```{r Forecast Function}
myforecast <- function(HDD_QTY, date, k, forecastPeriodLen){
  temp_data <- na.omit(unique(data.frame(ds = date, y = HDD_QTY)))
  n <- length(date)
  
  # mape_fcastPeriod_values <- matrix(NA,n-k-forecastPeriodLen+1,1)
  # MAPE_Quarterly_fcastPeriod <- matrix(NA,n-k-forecastPeriodLen+1,1)
  
  CV.Errors <- data.frame()
  for (i in seq(1,(n-k-forecastPeriodLen+1),4)){
    CV.Errors <- rbind(CV.Errors,mycrossvalidation(i,temp_data,k,forecastPeriodLen))
  }

  # Weekly Attainment Calculation
  MAPE_CFG_weekly <- mean(CV.Errors[,1],na.rm = TRUE)
  # Quarterly Attainment Calculation 
  MAPE_CFG_Quarterly <- mean(CV.Errors[,2],na.rm = TRUE)
  # TO avoid the big impact of the abnormal data
  MAPE_CFG_Quarterly_median <- median(CV.Errors[,2],na.rm = TRUE)
  
  return(c(MAPE_CFG_weekly,MAPE_CFG_Quarterly,MAPE_CFG_Quarterly_median))
}
```


```{r Run the fuctions parallel}
HDD_Weekly$date <- ymd(HDD_Weekly$Fiscal_Wk_End_Date)
CFGgroups <- levels(factor(HDD_Weekly$CFG))
len <- length(CFGgroups)
#len <- 2 # fOr quick test

forecastPeriodLen = 24 # Forecast for half a year
# The minimum number of observations for a training set
k <- 72 # using one year and a half

#  .export=c('HDD_Weekly')
Results <- foreach(i_CFG=1:len, .combine=rbind, .packages=c('tidyverse','prophet','Rcpp','ggthemes','lubridate','tidyquant')) %dopar% {
  # For each CFG
  temp_data <- HDD_Weekly %>% filter(CFG==CFGgroups[i_CFG])
  # If the total HDD demand after 2017-02-03 is more than 1000, try forecasting.
  if (sum(filter(temp_data,Fiscal_Wk_End_Date>='2017-02-03')$HDD_QTY)>1000){
     # If there are more than 24 available Weeks, try forecasting.
     if (nrow(temp_data)>(k+forecastPeriodLen)){
       # Forecast
       myforecast(temp_data$HDD_QTY, temp_data$date, k, forecastPeriodLen)
     }
   }
}

dimnames(Results)[[2]] <- c('MAPE_weekly','MAPE_Quarterly','MAPE_Quarterly_median')
Results <- data.frame(Results)
indx <- c('MAPE_weekly','MAPE_Quarterly','MAPE_Quarterly_median')
Results[indx] <- lapply(Results[indx], function(x) as.numeric(as.character(x)))

num_CFG_valid <- nrow(Results) # The total number of CFGs were forecasted.

Attainment_week <- nrow(Results %>% filter(MAPE_weekly<=25))
Attainment_Month <- nrow(Results %>% filter(MAPE_Quarterly<=25))
Attainment_Month2 <- nrow(Results %>% filter(MAPE_Quarterly_median<=25))

Attainment_weekRate = Attainment_week/num_CFG_valid
print(percent(Attainment_weekRate))

Attainment_MonthRate = Attainment_Month/num_CFG_valid
print(percent(Attainment_MonthRate))

Attainment_MonthRate2 = Attainment_Month2/num_CFG_valid
print(percent(Attainment_MonthRate2))

MAPE <- lapply(Results,mean)
MAPE_median <- lapply(Results,median)
print(MAPE)
print(MAPE_median)
```

```{r close the clusters}
# for Parallel package
stopImplicitCluster()
```

```{r Debug, eval=FALSE, include=FALSE}
i_CFG <- 1
temp_data <- HDD_Weekly %>% filter(CFG==CFGgroups[i_CFG])
HDD_QTY <- temp_data$HDD_QTY
date <- temp_data$date

temp_data <- na.omit(unique(data.frame(ds = date, y = HDD_QTY)))
n <- length(date)

mape_fcastPeriod_values <- matrix(NA,n-k-forecastPeriodLen+1,1)
MAPE_Quarterly_fcastPeriod <- matrix(NA,n-k-forecastPeriodLen+1,1)

CV.Errors <- data.frame()
for (i in seq(1,(n-k-forecastPeriodLen+1),4)){
  CV.Errors <- rbind(CV.Errors,mycrossvalidation(i,temp_data,k,forecastPeriodLen))
}

# Weekly Attainment Calculation
MAPE_CFG_weekly <- mean(CV.Errors[,1],na.rm = TRUE)
# Quarterly Attainment Calculation 
MAPE_CFG_Quarterly <- mean(CV.Errors[,2],na.rm = TRUE)
# TO avoid the big impact of the abnormal data
MAPE_CFG_Quarterly_median <- median(CV.Errors[,2],na.rm = TRUE)

print(c(MAPE_CFG_weekly,MAPE_CFG_Quarterly,MAPE_CFG_Quarterly_median))
```

